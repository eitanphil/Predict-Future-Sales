{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training method: Neural Network, XGBoost, LGBoosting, Liniar Regression\n",
    "### The most important features are: \n",
    "\n",
    "### How long it takes to train your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "from workalendar.europe import Russia\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import gc\n",
    "from itertools import product\n",
    "from datetime import date\n",
    "import time\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, BayesianRidge, Ridge, Lasso\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import lightgbm as lgb\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost import plot_importance\n",
    "\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def downcast_dtypes(df):\n",
    "    '''\n",
    "        Changes column types in the dataframe: \n",
    "                \n",
    "                `float64` type to `float32`\n",
    "                `int64`   type to `int32`\n",
    "    '''\n",
    "    \n",
    "    # Select columns to downcast\n",
    "    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n",
    "    int_cols =   [c for c in df if df[c].dtype == \"int64\"]\n",
    "    \n",
    "    # Downcast\n",
    "    df[float_cols] = df[float_cols].astype(np.float32)\n",
    "    df[int_cols]   = df[int_cols].astype(np.int32)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def mean_enc(all_data, group_cols, target_col, name, fill_c):\n",
    "    gb = all_data[group_cols + [target_col]].groupby(group_cols,as_index=False).agg({target_col:{name + '_mean':'mean'}})\n",
    "    #gb.reset_index(inplace=True)\n",
    "    gb.columns = [col[0] if col[-1] == '' else col[-1] for col in gb.columns.values]\n",
    "    all_data = pd.merge(all_data, gb, how='left', on=group_cols).fillna(fill_c)\n",
    "    return all_data\n",
    "  \n",
    "def mean_KFenc(data, enc_col, target_col, n_splits):\n",
    "    data= data.copy()\n",
    "    y= data[target_col]; X= data[enc_col]\n",
    "    new_col= enc_col + '_mean_skf'\n",
    "    data[new_col]= fill_c\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle= True, random_state=0)\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        enc= data.loc[train_index].groupby(enc_col)[target_col].mean()\n",
    "        data[new_col].loc[test_index]= data[enc_col].loc[test_index].map(enc)    \n",
    "    data[new_col].fillna(data[new_col].mean(), inplace=True)\n",
    "    return data\n",
    "\n",
    "\n",
    "def mean_expand_enc(all_data, enc_col, target_col):\n",
    "    #gb= all_data.groupby(enc_col)[target_col]\n",
    "    new_name= enc_col + '_expand_mean'\n",
    "    all_data['cs']= all_data.groupby(enc_col)[target_col].cumsum() - all_data[target_col]\n",
    "    all_data['ct']= all_data.groupby(enc_col)[target_col].cumcount()\n",
    "    all_data[]= all_data['cs'] / all_data['ct']\n",
    "    all_data[new_name].fillna(all_data[new_name].mean(), inplace=True)\n",
    "    return all_data\n",
    "\n",
    "\n",
    "\n",
    "def feature_lags(all_data, time_col, inx_cols, name, lags, fill_c):  \n",
    "    for lag in lags:\n",
    "        d= all_data[inx_cols + [name]].copy().drop_duplicates(subset=inx_cols)\n",
    "        d[time_col] += lag\n",
    "        d= d.rename(columns= {name: name + '_previous_' + str(lag)})\n",
    "        all_data = pd.merge(all_data, d, how='left', on=inx_cols).fillna(fill_c)\n",
    "    del d\n",
    "    return all_data\n",
    "\n",
    "def first_last(all_data, p_cols, reference, name, fl):\n",
    "    gb= all_data[p_cols + [reference]].groupby(p_cols, as_index=False)\n",
    "    #gb.reset_index(inplace=True)\n",
    "    if fl== 'first': \n",
    "        a=gb.first()\n",
    "    if fl== 'last':\n",
    "        a=gb.last()\n",
    "    a= a.rename(columns={reference:fl + '_' + name})\n",
    "    all_data = pd.merge(all_data, a, how='left', on=p_cols)\n",
    "    if fl== 'first': \n",
    "        all_data['delta_' + fl + '_' + name]= (all_data[reference] - all_data[fl + '_' + name]).fillna(0)\n",
    "    if fl== 'last': \n",
    "        all_data['delta_' + fl + '_' + name]= (all_data[reference] - all_data[fl + '_' + name]).fillna(33)\n",
    "    return all_data\n",
    "\n",
    "fill_c= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read_data\n",
    "path= 'Data/'\n",
    "t          = pd.read_csv(path + 'sales_train.csv.gz')\n",
    "test            = pd.read_csv(path + 'test.csv')\n",
    "items           = pd.read_csv(path + 'items.csv')\n",
    "item_cats = pd.read_csv(path + 'item_categories.csv')\n",
    "shops           = pd.read_csv(path + 'shops.csv')\n",
    "\n",
    "t['date']= pd.to_datetime(t['date'], format= '%d.%m.%Y')\n",
    "t= t.sort_values('date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input data EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shops.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking whether there ares new items in test data.\n",
    "col= 'item_id'\n",
    "new_items=len(list(set(test[col]) - set(test[col]).intersection(set(t[col]))))\n",
    "\n",
    "train_items= t[col].unique()\n",
    "test_items= test[col].unique()\n",
    "print('Number of train items: ', len(train_items))\n",
    "print('Number of test items: ', len(test_items))\n",
    "print('Number of new items in test: ', new_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking wether there are new shops in test data.\n",
    "col= 'shop_id'\n",
    "new_shops=set(test[col]).difference(set(t[col]))\n",
    "new_shops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "difference_shops=set(t[col]).difference(set(test[col]))\n",
    "difference_shops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_shops= sorted(t[col].unique())\n",
    "test_shops= sorted(test[col].unique())\n",
    "print('train shops: ', train_shops)\n",
    "print('test shops: ', test_shops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shops\n",
    "#### There seems to be a duplications of shops 0 with 57, 1 with 58 & 10 with 11. Since that shops 0, 1 & 11 aren't appearing in test data, then 0 should be convrted to 57, 1 to 58 & 10 to 11."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gb = shops.groupby('shop_name')\n",
    " \n",
    "gb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gb = items.groupby('item_name')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## item_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "item_cats.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cleaning and ordering based on EDA\n",
    "\n",
    "# removing outliers\n",
    "t = t[t['item_price']<50000]\n",
    "t = t[t['item_cnt_day']<500]\n",
    "\n",
    "# \n",
    "\n",
    "t.loc[t['shop_id'] == 0, 'shop_id'] = 57\n",
    "test.loc[test.shop_id == 0, 'shop_id'] = 57\n",
    "\n",
    "t.loc[t['shop_id'] == 1, 'shop_id'] = 58\n",
    "test.loc[test.shop_id == 1, 'shop_id'] = 58\n",
    "\n",
    "t.loc[t['shop_id'] == 10, 'shop_id'] = 11\n",
    "test.loc[test.shop_id == 10, 'shop_id'] = 11\n",
    "\n",
    "shops.loc[shops.shop_name == 'Сергиев Посад ТЦ \"7Я\"', 'shop_name'] = 'СергиевПосад ТЦ \"7Я\"'\n",
    "shops['city'] = shops['shop_name'].str.split(' ').map(lambda x: x[0])\n",
    "shops.loc[shops.city == '!Якутск', 'city'] = 'Якутск'\n",
    "shops['city_code'] = LabelEncoder().fit_transform(shops['city'])\n",
    "shops = shops[['shop_id','city_code']]\n",
    "\n",
    "items.drop(['item_name'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "item_cats['split'] = item_cats['item_category_name'].str.split('-')\n",
    "item_cats['type'] = item_cats['split'].map(lambda x: x[0].strip())\n",
    "item_cats['type_code'] = LabelEncoder().fit_transform(item_cats['type'])\n",
    "# if subtype is nan then type\n",
    "item_cats['subtype'] = item_cats['split'].map(lambda x: x[1].strip() if len(x) > 1 else x[0].strip())\n",
    "item_cats['subtype_code'] = LabelEncoder().fit_transform(item_cats['subtype'])\n",
    "item_cats = item_cats[['item_category_id','type_code', 'subtype_code']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Check leakedges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(t.index, t['item_cnt_day'], s=0.5); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(t['shop_id'], t['item_cnt_day'], s=0.5); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(test['shop_id'], test['ID'], s=0.5);plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "plt.scatter(test['shop_id'], test['item_id'], s=0.5);plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "plt.scatter(test['item_id'], test['ID'], s=0.5);plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are some patterns that require further investigation in the future. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tr_shops= list(t['shop_id'].unique())\n",
    "ts_shops= list(test['shop_id'].unique())\n",
    "tr_items= list(t['item_id'].unique())\n",
    "ts_items= list(test['item_id'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create basic dataset for processing\n",
    "#### For each item in each shop I create an instance for each month. This way there will be a continious data of each specific item for creating features and training the model.  \n",
    "#### Test data will be concatenated to train data for easier processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create grid\n",
    "index_cols = ['shop_id', 'item_id', 'date_block_num']\n",
    "\n",
    "grid = [] \n",
    "for block_num in t['date_block_num'].unique():\n",
    "    cur_shops = t.loc[t['date_block_num'] == block_num, 'shop_id'].unique()\n",
    "    cur_items = t.loc[t['date_block_num'] == block_num, 'item_id'].unique()\n",
    "    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))\n",
    "\n",
    "# Turn the grid into a dataframe\n",
    "grid = pd.DataFrame(np.vstack(grid), columns = index_cols, dtype=np.int32)\n",
    "\n",
    "\n",
    "grid['ID']= -1\n",
    "\n",
    "#grid= grid[grid['shop_id'].isin(ts_shops)]\n",
    "#grid= grid[grid['item_id'].isin(ts_items)]\n",
    "\n",
    "\n",
    "g= grid.shape\n",
    "print('grid', g)\n",
    "\n",
    "\n",
    "# Preparing test set\n",
    "test['date_block_num'] = 34\n",
    "test['date_block_num'] = test['date_block_num'].astype(np.int8)\n",
    "test['shop_id'] = test['shop_id'].astype(np.int8)\n",
    "test['item_id'] = test['item_id'].astype(np.int16)\n",
    "\n",
    "keep= len(grid)+len(test)\n",
    "print('keep: ', keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating grid & test data\n",
    "all_data= pd.concat([grid, test], axis=0, ignore_index=True)#, sort=False)\n",
    "all_data.fillna(0, inplace=True)\n",
    "del grid\n",
    "\n",
    "# Merging Shops, Categories, items \n",
    "all_data = pd.merge(all_data, shops, on=['shop_id'], how='left')\n",
    "all_data = pd.merge(all_data, items, on=['item_id'], how='left')\n",
    "all_data = pd.merge(all_data, item_cats, on=['item_category_id'], how='left')\n",
    "all_data['item_category_id'] = all_data['item_category_id'].astype(np.int8)\n",
    "all_data['city_code'] = all_data['city_code'].astype(np.int8)\n",
    "all_data['type_code'] = all_data['type_code'].astype(np.int8)\n",
    "all_data['subtype_code'] = all_data['subtype_code'].astype(np.int8)\n",
    "\n",
    "print(all_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate target data\n",
    "#### I will clip target to 0-20 for better convergance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target of current month\n",
    "group_cols= ['shop_id', 'item_id', 'date_block_num']\n",
    "gb = t.groupby(group_cols,as_index=False).agg({'item_cnt_day':{'target':'sum'}})\n",
    "gb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values]\n",
    "all_data = pd.merge(all_data, gb, how='left', on=group_cols).fillna(0)\n",
    "\n",
    "# Clipping\n",
    "all_data['target']= all_data['target'].clip(0, 20, axis=0).astype(np.int8)\n",
    "print(all_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features\n",
    "#### * Months & year.\n",
    "#### * Months since firs & last sales.\n",
    "#### * Mean price for each month, price lags & trends.\n",
    "#### * Working days and holydays in each month.\n",
    "#### * Mean encoding using skf strategy.\n",
    "#### * Target lags, trends & estimations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " '''\n",
    "More to investigate: \n",
    "1. Shops location relative to other locations. Same as in video.\n",
    "2. Text from item_name. need to encode by characteristics, like BD, D, etc..\n",
    "3. TF-IDF.\n",
    "4. Model for new features. For example those which not appear at the first dbn.\n",
    "5. Analyze sales along years. find the average factor between sales.\n",
    "6. investigate encoding to pairs and with inx_cols\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate month & year\n",
    "all_data['month']= (all_data['date_block_num'] % 12 + 1).astype(np.int32)\n",
    "all_data['year']= (all_data['date_block_num'] // 12 + 2013).astype(np.int32)\n",
    "\n",
    "# months since first & last sale\n",
    "reference= 'date_block_num'\n",
    "\n",
    "p_cols=['shop_id', 'item_id']; name= 'shop_item'; fl= 'first'\n",
    "all_data= first_last(all_data, p_cols, reference, name, fl)\n",
    "\n",
    "p_cols=['shop_id', 'item_id']; name= 'shop_item'; fl= 'last'\n",
    "all_data= first_last(all_data, p_cols, reference, name, fl)\n",
    "\n",
    "p_cols=['shop_id']; name= 'shop'; fl= 'first'\n",
    "all_data= first_last(all_data, p_cols, reference, name, fl)\n",
    "\n",
    "p_cols=['shop_id']; name= 'shop'; fl= 'last'\n",
    "all_data= first_last(all_data, p_cols, reference, name, fl)\n",
    "\n",
    "p_cols=['item_id']; name= 'item'; fl= 'first'\n",
    "all_data= first_last(all_data, p_cols, reference, name, fl)\n",
    "\n",
    "p_cols=['item_id']; name= 'item'; fl= 'last'\n",
    "all_data= first_last(all_data, p_cols, reference, name, fl) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prices\n",
    "time_col= 'date_block_num'\n",
    "inx_cols= ['shop_id', 'item_id', 'date_block_num']\n",
    "lags= [1, 2, 3, 4, 6, 9, 12]\n",
    "gb= t[inx_cols + ['item_price']].groupby(inx_cols, as_index=False).mean()\n",
    "all_data= pd.merge(all_data, gb, how= 'left', on=inx_cols) \n",
    "all_data= feature_lags(all_data, time_col, inx_cols, 'item_price', lags, fill_c)\n",
    "for l in lags[1:]:\n",
    "    all_data['item_price_trend_' + str(l)]= all_data['item_price_previous_1'] - all_data['item_price_previous_' + str(l)]\n",
    "all_data.drop('item_price', axis=1, inplace=True)\n",
    "print('prices')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Time features    \n",
    "days_in_month = pd.Series([31,28,31,30,31,30,31,31,30,31,30,31])\n",
    "all_data['month'] -=1\n",
    "all_data['days'] = all_data['month'].map(days_in_month).astype(np.int8)\n",
    "all_data['month'] +=1\n",
    "cal = Russia()\n",
    "\n",
    "y= 2014\n",
    "all_data['w_days']= 0\n",
    "\n",
    "for m, last in enumerate(days_in_month):\n",
    "    all_data['w_days'][all_data['month']==m+1]= cal.get_working_days_delta(date(y, m+1, 1), date(y, m+1, last))\n",
    "all_data['h_days']= all_data['days'] - all_data['w_days'] - 8\n",
    "gc.collect();\n",
    "print(all_data.shape)\n",
    "\n",
    "\n",
    "all_data['date_block_num'] += 1\n",
    "\n",
    "print('time')\n",
    "print(all_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Expand Mean Encoding  \n",
    "enc_cols= ['shop_id', 'item_id', 'item_category_id', 'type_code', 'subtype_code', 'city_code']\n",
    "#inx_cols= ['shop_id', 'item_id', 'date_block_num']\n",
    "target_col= 'target'\n",
    "\n",
    "for enc_col in enc_cols:\n",
    "    all_data= mean_expand_enc(all_data, enc_col, target_col)\n",
    "    \n",
    "all_data.drop(['cs', 'ct'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SKF Mean Encoding    \n",
    "inx_cols= ['shop_id', 'item_id', 'date_block_num']\n",
    "time_col= 'date_block_num'\n",
    "target_col= 'target'\n",
    "\n",
    "skf_cols= ['shop_id', 'item_id', 'item_category_id', 'type_code', 'subtype_code', 'city_code']#, 'month', 'year']\n",
    "data1= all_data[all_data['ID']==-1]\n",
    "data2= all_data[all_data['ID']>-1]\n",
    "for skf_col in skf_cols:\n",
    "    data1= mean_KFenc(data1, skf_col, target_col, n_splits=10)\n",
    "    data2= pd.merge(data2, data1[[skf_col, skf_col + '_mean_skf']].drop_duplicates(subset=skf_col, keep='last'), how='left', on=skf_col)\n",
    "all_data= pd.concat([data1, data2], axis=0)\n",
    "\n",
    "print('skf_enc')\n",
    "print('all_data shape: ', all_data.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairs Mean Encoding\n",
    "group_cols= ['shop_id', 'item_id']; name= 'target_shop_item'\n",
    "all_data= mean_enc(all_data, group_cols, target_col, name, fill_c)\n",
    "\n",
    "group_cols= ['shop_id', 'city_code']; name= 'target_shop_city'\n",
    "all_data= mean_enc(all_data, group_cols, target_col, name, fill_c)\n",
    "\n",
    "group_cols= ['shop_id', 'item_category_id']; name= 'target_shop_cat'\n",
    "all_data= mean_enc(all_data, group_cols, target_col, name, fill_c)\n",
    "\n",
    "group_cols= ['shop_id', 'type_code']; name= 'target_shop_type'\n",
    "all_data= mean_enc(all_data, group_cols, target_col, name, fill_c)\n",
    "\n",
    "group_cols= ['shop_id', 'subtype_code']; name= 'target_shop_subtype'\n",
    "all_data= mean_enc(all_data, group_cols, target_col, name, fill_c)\n",
    "\n",
    "print('mean encoding')\n",
    "print('all_data shape: ', all_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target lags\n",
    "group_cols= ['shop_id', 'item_id', 'date_block_num']\n",
    "lags= [1, 2, 3, 4, 5, 6, 9, 12]\n",
    "for i in lags:       \n",
    "    d= all_data[group_cols + ['target']].copy().drop_duplicates()\n",
    "    d['date_block_num'] += i\n",
    "    d= d.rename(columns= {'target':'target_previous_' + str(i)})\n",
    "    all_data = pd.merge(all_data, d, how='left', on=group_cols).fillna(0)\n",
    "del d\n",
    "print('target lags')\n",
    "print('all_data shape: ', all_data.shape)\n",
    "\n",
    "# Target trend\n",
    "inx_cols= ['shop_id', 'item_id', 'date_block_num']\n",
    "time_col= 'date_block_num'\n",
    "target_col= 'target'\n",
    "trend_lags= [2, 3, 4, 5, 6, 9, 12]\n",
    "for l in trend_lags:\n",
    "    all_data['target_trend_' + str(l)]= all_data['target_previous_1'] - all_data['target_previous_' + str(l)]\n",
    "    all_data['target_trend_' + str(l)].fillna(-1)\n",
    "name= 'target_trend_2'\n",
    "\n",
    "all_data= feature_lags(all_data, time_col, inx_cols, name, [11, 12, 13], fill_c)\n",
    "\n",
    "print('target trends')\n",
    "print('all_data shape', all_data.shape)\n",
    "\n",
    "#Trying to estimate\n",
    "all_data['tar11']= all_data['target_previous_1'] * (1 + all_data['target_trend_2_previous_11'] / all_data['target_previous_12']).replace([np.inf, -np.inf, np.nan], 1)\n",
    "all_data['tar12']= all_data['target_previous_1'] * (1 + all_data['target_trend_2_previous_12'] / all_data['target_previous_12']).replace([np.inf, -np.inf, np.nan], 1)\n",
    "all_data['tar13']= all_data['target_previous_1'] * (1 + all_data['target_trend_2_previous_13'] / all_data['target_previous_12']).replace([np.inf, -np.inf, np.nan], 1)\n",
    "all_data['tar111']= all_data['target_previous_1'] + all_data['target_trend_2_previous_11']\n",
    "all_data['tar112']= all_data['target_previous_1'] + all_data['target_trend_2_previous_12']\n",
    "all_data['tar113']= all_data['target_previous_1'] + all_data['target_trend_2_previous_13']\n",
    "    \n",
    "print('estimate')\n",
    "print('all_data shape: ', all_data.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_data = downcast_dtypes(all_data)\n",
    "print('all_data shape: ', all_data.shape);\n",
    "print(all_data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data.to_csv('all_data.csv', index=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "More to investigate: \n",
    "1. Analyze differences between features distribution in train & test datas.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cut= 14\n",
    "train_data= all_data[all_data['ID']==-1]\n",
    "\n",
    "y= train_data[['target', 'date_block_num']][train_data['date_block_num'] >= cut]\n",
    "\n",
    "X= train_data.drop(['ID', 'target'], axis=1)\n",
    "X= X[X['date_block_num'] >= cut]\n",
    "dbn= X['date_block_num']\n",
    "X= X.drop('date_block_num', axis=1)\n",
    "\n",
    "\n",
    "test_data=  all_data[all_data['ID'] > -1].drop(['target', 'date_block_num'], axis=1)\n",
    "ID= test_data['ID']\n",
    "test_data.drop('ID', axis=1, inplace=True)\n",
    "\n",
    "del all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols= ['target_shop_item_mean', 'target_shop_city_mean', 'target_shop_cat_mean','target_shop_type_mean', \n",
    "       'target_shop_subtype_mean', 'item_id', 'shop_id', 'city_code', 'item_category_id', 'type_code',\n",
    "       'subtype_code', 'month', 'year', 'item_id_mean_skf', 'shop_id_mean_skf']\n",
    "X= X.drop(cols, axis=1)\n",
    "test_data= test_data.drop(cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(t['item_id'], t['item_price'], s=0.5); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Advanced features\n",
    "#### * Selecting data from month 14\n",
    "#### * Split between data for modeling and data for submission.\n",
    "#### * Extract RF leaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RF features\n",
    "RFmodel = RandomForestRegressor(max_depth=3, min_samples_split=1000, random_state=0,n_estimators=3)\n",
    "RFmodel.fit(X, y)\n",
    "\n",
    "a= RFmodel.apply(X)\n",
    "b= RFmodel.apply(test_data)\n",
    "\n",
    "RFX= pd.DataFrame(data=a, columns=['RF1', 'RF2', 'RF3'])\n",
    "RFtest= pd.DataFrame(data=b, columns=['RF1', 'RF2', 'RF3'])\n",
    "\n",
    "feature_importances = pd.DataFrame(RFmodel.feature_importances_, \n",
    "                                   index = X.columns, \n",
    "                                   columns=['importance']).sort_values('importance', ascending=False)\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RFX.to_csv('RFX.csv')\n",
    "RFtest.to_csv('RFtest.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols= X.columns.tolist()\n",
    "scaler = MinMaxScaler()\n",
    "#scaler = StandardScaler()\n",
    "XS= scaler.fit_transform(X)\n",
    "test_dataS= scaler.transform(test_data)\n",
    "\n",
    "scaler1 = MinMaxScaler(feature_range=(0, 1))\n",
    "yS= scaler1.fit_transform(y.values.reshape(-1, 1))\n",
    "\n",
    "\n",
    "XS= pd.DataFrame(data=XS, columns=cols)\n",
    "test_dataS= pd.DataFrame(data=test_dataS, columns=cols)\n",
    "yS= pd.DataFrame(data=yS, columns=['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "KNNmodel= KNeighborsRegressor(n_neighbors=5)\n",
    "KNNmodel.fit(XS, y)\n",
    "distX= KNNmodel.kneighbors(XS)\n",
    "dist_test= KNNmodel.kneighbors(test_dataS)\n",
    "\n",
    "knncols=[]\n",
    "for i in distX.shape[1]:\n",
    "    knncols.append('knn5' + str(i))\n",
    "    \n",
    "distX= pd.DataFrame(data=distX, columns= knncols)\n",
    "dist_test= pd.DataFrame(data=dist_test, columns=knncols)\n",
    "print('knn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "distX.to_csv('distX.csv')\n",
    "dist_test.to_csv('dist_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Training\n",
    "#### * Split between train and validation data.\n",
    "#### * Modeling with Light Gradient Boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X= pd.read_csv('X_data.csv');\n",
    "y= pd.read_csv('y_data.csv');\n",
    "test_data= pd.read_csv('test_data.csv');\n",
    "dbn= pd.read_csv('date_block_num.csv')\n",
    "RFX= pd.read_csv('RFX.csv')\n",
    "RFtest= pd.read_csv('RFtest.csv')\n",
    "#distX= pd.read_csv('distX.csv')\n",
    "#dist_test= pd.read_csv('dist_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X= pd.read_csv('X_data.csv');\n",
    "y= pd.read_csv('y_data.csv');\n",
    "test_data= pd.read_csv('test_data.csv');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xrf= pd.concat([XS, RFX], axis=1)\n",
    "test_data_rf= pd.concat([test_dataS, RFtest], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ens_models(X_train, y_train, X_valid):\n",
    "\n",
    "\n",
    "    tlevel1= []; vlevel1= []\n",
    "    print('lgb')\n",
    "    lgb_params1 = {'feature_fraction': 0.7,'metric': 'rmse', 'nthread':12, 'min_data_in_leaf': 2**15, \n",
    "                      'bagging_fraction': 0.7, 'learning_rate': 0.05, 'objective': 'rmse',\n",
    "                      'bagging_seed': 2**9, 'num_leaves': 2**9,'bagging_freq':1,'verbose':0}\n",
    "\n",
    "    lgb_params2 = {'feature_fraction': 0.7,'metric': 'rmse', 'nthread':12, 'min_data_in_leaf': 2**11, \n",
    "                  'bagging_fraction': 0.7, 'learning_rate': 0.07, 'objective': 'rmse',\n",
    "                  'bagging_seed': 2**7, 'num_leaves': 2**9,'bagging_freq':1,'verbose':0}\n",
    "\n",
    "    lgb_params3 = {'feature_fraction': 0.7,'metric': 'rmse', 'nthread':12, 'min_data_in_leaf': 2**7, \n",
    "                  'bagging_fraction': 0.7, 'learning_rate': 0.05, 'objective': 'rmse',\n",
    "                  'bagging_seed': 2**5, 'num_leaves': 2**11,'bagging_freq':1,'verbose':0}\n",
    "\n",
    "    for n, lgb_params in enumerate ([lgb_params1, lgb_params2, lgb_params3]):\n",
    "        model = lgb.train(lgb_params, lgb.Dataset(X_train, label=y_train), 30)\n",
    "        #ax = lgb.plot_importance(model, figsize=(15, 15));plt.show()\n",
    "        \n",
    "        t_p= model.predict(X_train)\n",
    "        v_p= model.predict(X_valid)\n",
    "        vlevel1.append(v_p)\n",
    "    \n",
    "    print('RF')\n",
    "    RF1= RandomForestRegressor(max_depth= 3, min_samples_split= 1000, min_samples_leaf= 50, random_state= 0, n_estimators= 10)\n",
    "   \n",
    "    for n, RFmodel in enumerate ([RF1]):\n",
    "        RFmodel.fit(X_train, y_train)\n",
    "     \n",
    "    \n",
    "        t_p= model.predict(X_train)\n",
    "        tlevel1.append(t_p)\n",
    "        v_p= RFmodel.predict(X_valid)\n",
    "        vlevel1.append(v_p)\n",
    "    \n",
    "    print('LR')    \n",
    "    LRmodel= LinearRegression()\n",
    "    Lmodel= Lasso()\n",
    "    Rmodel= Ridge()\n",
    "    BRmodel= BayesianRidge()\n",
    "    for n, model in enumerate ([LRmodel, Lmodel, Rmodel, BRmodel]):\n",
    "        model.fit(X_train, y_train)\n",
    "    \n",
    "        t_p= model.predict(X_train)\n",
    "        tlevel1.append(t_p.squeeze())\n",
    "        v_p= model.predict(X_valid)\n",
    "        vlevel1.append(v_p.squeeze())\n",
    "    \n",
    "    return vlevel1, tlevel1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XS.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predicted_train= Xrf\n",
    "predicted_train['date_bulk']= dbn.values.squeeze()\n",
    "predicted_test= test_data_rf\n",
    "yy= y.copy()\n",
    "yy.columns= ['target', 'date_bulk']\n",
    "#yy['date_bulk']= dbn.values.squeeze()\n",
    "last_bulk= predicted_train['date_bulk'].max()\n",
    "\n",
    "for n, k in enumerate ([30, last_bulk-1, last_bulk]): #[30, last_bulk-1, last_bulk]\n",
    "    print(k)\n",
    "    XX= predicted_train.copy()\n",
    "    df_test= predicted_test.copy()\n",
    "\n",
    "    predicted_train= pd.DataFrame()\n",
    "    #k= 24    \n",
    "    for m in range(k, last_bulk + 1):\n",
    "        print(m)\n",
    "        chunk= np.arange(XX['date_bulk'].min(), m + 1)\n",
    "        X_train= XX[XX['date_bulk'].isin(chunk)]\n",
    "        y_train= yy[yy['date_bulk'].isin(chunk)]\n",
    "\n",
    "        if m < last_bulk:\n",
    "            X_valid= XX[XX['date_bulk']== m + 1]\n",
    "\n",
    "            vlevel1, _= ens_models(X_train.drop('date_bulk', axis=1), y_train.drop('date_bulk', axis=1), X_valid.drop('date_bulk', axis=1))\n",
    "\n",
    "            df= pd.DataFrame(data= np.array(vlevel1).T, columns= np.arange(1, len(vlevel1) + 1))\n",
    "            df['date_bulk']= m + 1\n",
    "\n",
    "            predicted_train= pd.concat([predicted_train, df], axis=0)\n",
    "        else:\n",
    "            X_valid= df_test\n",
    "\n",
    "            vlevel1, tlevel1= ens_models(X_train.drop('date_bulk', axis=1), y_train.drop('date_bulk', axis=1), X_valid)\n",
    "            vlevel1= np.array(vlevel1).T\n",
    "            tlevel1= np.array(tlevel1).T\n",
    "            predicted_test= pd.DataFrame(data= vlevel1, columns= np.arange(1, vlevel1.shape[1] + 1))\n",
    "\n",
    "\n",
    "BRmodel1= BayesianRidge()\n",
    "BRmodel1.fit(tlevel1, y_train.drop('date_bulk', axis=1))\n",
    "pr1= BRmodel1.predict(tlevel1).clip(0,20)\n",
    "pr2= BRmodel1.predict(vlevel1).clip(0,20)\n",
    "            \n",
    "print('dbn 34 before final level (mean) rmse: ', np.sqrt(mean_squared_error(y['target'][yy['date_bulk']==34], df.drop('date_bulk', axis=1).mean(axis=1).clip(0,20))));\n",
    "print('dbn 34 before final level rmse: ', np.sqrt(mean_squared_error(y['target'][yy['date_bulk']==34], df[4].clip(0,20))));\n",
    "\n",
    "print('all modeling data rmse: ', np.sqrt(mean_squared_error(y['target'].iloc[-len(pr1):], pr1)));\n",
    "print('dbn 34 rmse: ', np.sqrt(mean_squared_error(y['target'][yy['date_bulk']==34], pr1[-len(y['target'][yy['date_bulk']==34]):])));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d= df.copy()\n",
    "d['target']= y['target'][yy['date_bulk']==34].values\n",
    "d.head(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pridiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_data['ID']= ID.values\n",
    "ID= test_data[['ID']];\n",
    "ID['ID']= ID['ID'].astype(int) \n",
    "\n",
    "#ID['item_cnt_month']= predicted_test.mean(axis=1).values;\n",
    "#ID['item_cnt_month']= predicted_test[3].values; #!!!!!\n",
    "ID['item_cnt_month']= pr2\n",
    "ID['item_cnt_month']= ID['item_cnt_month'].clip(0,20)\n",
    "ID.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID['item_cnt_month'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ID.to_csv('SUBMISSION07MAY_0.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
